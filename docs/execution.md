# Pipeline Execution and Directory Structure

This document explains how to execute the data pipeline and provides a detailed reference for the project's directory structure.

## 1. Running the Pipeline

The project has two primary entry points: `run_smoke_test.py` for offline testing and `run_pipeline.py` for live online data collection. The easiest way to run the standard offline test is by using the provided `run_bootstrap.sh` script.

### Standard Offline Smoke Test

The repository includes a self-contained "smoke test" that processes a small, local bootstrap dataset to verify that all components are working correctly. To run it, execute the following command from the project root:

```bash
bash run_bootstrap.sh
```

This script performs two actions:
1.  It installs the necessary Python dependencies, including `akshare`.
2.  It executes `run_smoke_test.py`, which runs the full end-to-end pipeline in offline `replay` mode.

Upon successful completion, all processed data and exported files will be generated in the `data_silver/`, `data_gold/`, and `exports/` directories.

### Online Data Collection

For live data collection, use the `run_pipeline.py` script. This script is designed to run the pipeline in `online` mode, which uses the `HttpTransport` to fetch data from live sources.

```bash
python3 run_pipeline.py [--full-run] [--start-date <date>] [--end-date <date>]
```

#### Command-Line Arguments

-   `--full-run`: A flag that, when present, instructs the pipeline to generate a manifest for all configured interfaces over the specified date range. If this flag is omitted, the pipeline runs a small online test with a limited number of tasks.
-   `--start-date <date>`: Specifies the start date for a full run (e.g., `2024-01-01`). Defaults to `2024-01-01`.
-   `--end-date <date>`: Specifies the end date for a full run (e.g., `2024-01-31`). Defaults to `2024-01-31`.

**Important:** Before running in online mode, ensure you have set the necessary proxy environment variables as described in `AGENTS.md`.

#### Examples

-   **Run a small online test to ensure connectivity and proxy rotation:**
    ```bash
    python3 run_pipeline.py
    ```
-   **Run a full online data collection job for a specific date range:**
    ```bash
    python3 run_pipeline.py --full-run --start-date 2024-01-01 --end-date 2024-03-31
    ```

## 2. Directory Structure

The project is organized into the following directories, each with a specific purpose.

```
/
├── config/             # YAML configuration files for the pipeline.
├── data_raw/           # Raw, unprocessed data from sources.
├── data_silver/        # Cleaned, standardized data (intermediate layer).
├── data_gold/          # Final, model-ready features and labels.
├── docs/               # Project documentation.
├── exports/            # Final exported datasets (CPT, SFT, TXT) and stats.
├── inputs/             # Static input files, like CSI300 component lists.
├── manifests/          # Task lists and checkpoints for the pipeline.
├── src/                # All Python source code for the pipeline modules.
├── tests/              # Unit tests for the pipeline modules.
├── run_pipeline.py     # Entry point for online data collection.
├── run_smoke_test.py   # Entry point for the offline smoke test.
└── run_bootstrap.sh    # Convenience script to install deps and run the smoke test.
```

-   **`config/`**: Contains all YAML configuration files. This is where you define data interfaces, schemas, and other parameters.
-   **`data_raw/`**: Stores the raw, unmodified data fetched from sources. The `bootstrap/` subdirectory holds the static data for the offline smoke test.
-   **`data_silver/`**: The first layer of processed data. Data here is cleaned, normalized, and stored in an efficient format (Parquet), with one subdirectory per data interface.
-   **`data_gold/`**: The final, enriched data layer. The `features/` subdirectory contains the point-in-time feature table, and `labels/` contains the generated labels.
-   **`docs/`**: Contains all project documentation (what you are reading now).
-   **`exports/`**: The final output directory. It contains the model-ready datasets in `cpt/` and `sft/` formats, a human-readable `txt/` preview, and statistical reports in `stats/`.
-   **`inputs/`**: Holds static data files that are not source data but are required for pipeline logic, such as the list of stocks in the `CSI300.csv`.
-   **`manifests/`**: Stores the task manifests generated by the `ManifestGenerator`. These Parquet files list the atomic data fetching jobs for a given run.
-   **`src/`**: Contains all the core Python logic of the pipeline, broken down into modular files.
-   **`tests/`**: Contains unit tests to ensure the correctness of individual modules.
