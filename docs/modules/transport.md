# Module: `transport.py`

The `transport.py` module is responsible for the "T" in ETL (Extract, Transform, Load). It handles the data fetching stage of the pipeline. It contains two "transport" classes that adhere to a common interface, each designed for a different pipeline mode.

The `Orchestrator` selects which transport to use based on the `mode` parameter passed to its `run` method.

---

## Class: `ReplayTransport`

-   **Purpose**: This is the default transport layer, used in `replay` mode. It supports the "offline-first" development principle by "fetching" data from local files instead of making live network requests.
-   **Usage**: It is the core of the offline smoke test.

### `fetch(self, tasks_df: pd.DataFrame) -> List[Dict[str, Any]]`

-   **Inputs**:
    -   `tasks_df`: The manifest DataFrame generated by `ManifestGenerator`.
-   **Logic**:
    1.  It iterates through each task in the manifest.
    2.  For each task, it reads the local CSV file specified in `replay_path`.
    3.  It saves the data to the Parquet file specified in `output_path`.
-   **Output**: A list of dictionaries. Each dictionary contains the original `task` information and the `raw_file_path` pointing to the newly created Parquet file.

---

## Class: `HttpTransport`

-   **Purpose**: This is the transport layer for `online` mode. It is designed to fetch live data from `akshare` API endpoints. It is a more complex class that includes features necessary for robust, real-world data collection.
-   **Usage**: Used when running the pipeline via `run_pipeline.py`.

### Key Features

-   **Resumable Downloads (Checkpointing)**: This is the most critical feature for robust data collection. The transport reads a `manifests/checkpoints.parquet` file at the start of a run to identify and skip any tasks that have already been successfully completed. This ensures that if a long-running job is interrupted, it can be resumed from where it left off.
-   **Domain-Specific Rate Limiting**: To avoid being blocked, the transport enforces rate limits on a per-domain basis. It uses a thread-safe `TokenBucket` mechanism to ensure that the number of requests per second to any given domain (e.g., `eastmoney.com`) does not exceed the limits defined in `config/rate_limits.yaml`.
-   **Secure Proxy Rotation**: The transport loads Bright Data proxy credentials securely from environment variables. For each request, it generates a new random session ID to ensure a different proxy IP is used, which is essential for large-scale fetching.
-   **Concurrency and Retries**: The main `fetch` method uses a `ThreadPoolExecutor` to make multiple API calls concurrently. Failed requests are automatically retried with an exponential backoff delay.
-   **`akshare` Integration**: It maps the `interface_id` from the configuration to the actual callable function in the `akshare` library.

### `fetch(self, tasks_df: pd.DataFrame) -> List[Dict[str, Any]]`

-   **Inputs**:
    -   `tasks_df`: The manifest DataFrame. For online mode, this will contain `params` for the API calls instead of a `replay_path`.
-   **Logic**:
    1.  It first loads the set of already completed task IDs from `manifests/checkpoints.parquet`.
    2.  It filters the incoming `tasks_df` to exclude these completed tasks.
    3.  It submits the remaining pending tasks to a thread pool.
    4.  Each worker thread acquires a token from the appropriate domain's rate limiter, fetches the data, and upon success, saves the resulting DataFrame to `data_raw/` and appends the task ID to the checkpoint file.
-   **Output**: A list of dictionaries, where each dictionary contains the `task` metadata and the `raw_file_path` of the newly downloaded data.
